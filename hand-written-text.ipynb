{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Automate extraction of handwritten text from an image.**\nCNN Model - Kaggle\n---\n---\n\n**Name Of The Student\t\t:**\tViddesh Kamble\n\n**Internship Project Title\t\t:** Automate extraction of handwritten text from an image\n\n**Name of the Organization\t:** TCS iON\n\n**Name of the Industry Mentor\t:** Debashis Roy Sir\n\n**Name of the Institute\t\t:** B.K Birla college\n\n**Name of the Academic Mentor  :** Swapna Maâ€™am\n\n---\n---\n\n## **Introduction:**\nAn optical character recognition problem is basically a type of image-based sequence recognition problem. And for sequence recognition problem, most suited neural networks are recurrent neural networks(RNN) while for an image-based problem most suited are convolution neural networks(CNN). To cop up with the OCR problems we need to combine both of these CNN and RNN and use SoftMax.\n\n## **Steps being followed are as follows:**\nThe complete implementation of the project can be divided into the following major steps:\n\n**1.\tCollecting the Dataset.**\n\n**2.\tUploading the Dataset on directory and acessing it.**\n\n**3.\tPreprocessing the data.**\n\n**4.\tSplitting the datset into train, test and validation datsets.**\n\n**5.\tCreating the defining the model/network architecture.**\n\n**6.\tTraining the model.**\n\n**7.\tSaving the ML model.**\n\n**8.\tTesting the model.**\n\n**9.\tPrediction.**\n\n**10.\tPlotting the loss and accuracy plots.**\n\n\n## **Step 1: Collecting the Dataset**\nwe used IAM handwritten datset. This is good dataset with several preprocessing already done.\n\nTo download the dataset either you can directly download from [this link](https://www.kaggle.com/datasets/tejasreddy/iam-handwriting-top50) or use the following commands to download the data and unzip.\n\nMy data subset which I am using for training and testing having around 4899 images and 50 writers.","metadata":{}},{"cell_type":"code","source":"# # Dataset :- https://www.kaggle.com/datasets/tejasreddy/iam-handwriting-top50\n\n# # In Kaggle or Google drive add prefix \"!\" for pip command.\n\n# # Below are all the packages or requirements \n\n# # pip install numpy==1.24.3\n# # pip install Pillow==10.0.0\n# # pip install keras==2.13.1\n# # pip install tensorflow==2.13.0\n# # pip install scikit-learn==1.3.0\n# # pip install matplotlib==3.7.2\n# # pip install pandas==2.1.0\n# # pip install openpyxl==3.1.2\n\n# import numpy \n# import PIL\n# import tensorflow\n# import keras\n# import sklearn\n# import matplotlib\n# import pandas\n# import openpyxl\n# import sys\n\n\n# print(f\"Numpy version:- {numpy.__version__}\") # 1.24.3\n# print(f\"Pillow version:- {PIL.__version__}\") # 10.0.0\n# print(f\"Keras version:- {keras.__version__}\") # 2.13.1\n# print(f\"Tensorflow version:- {tensorflow.__version__}\") # 2.13.0\n# print(f\"Sklearn version:- {sklearn.__version__}\") # 1.3.0\n# print(f\"Matplotlib version:- {matplotlib.__version__}\") # 3.7.2\n# print(f\"Pandas version:- {pandas.__version__}\") # 2.1.0\n# print(f\"openpyxl version:- {openpyxl.__version__}\") # 3.1.2\n# print(f\"Python version:- {sys.version.split()[0]}\") # 3.10.12","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:09.538642Z","iopub.execute_input":"2023-09-26T03:18:09.539010Z","iopub.status.idle":"2023-09-26T03:18:09.544788Z","shell.execute_reply.started":"2023-09-26T03:18:09.538979Z","shell.execute_reply":"2023-09-26T03:18:09.543389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing all the necessary libraries needed for Automate extraction of handwritten text from an image\n\nimport os # Used for interacting with the operating system, such as navigating directories and file management.\nimport glob # Useful for file pattern matching and retrieving a list of filenames.\nimport numpy as np # A fundamental library for numerical computations and array manipulation.\n# from matplotlib import image as mpimg\nfrom matplotlib import pyplot as plt # For data visualization\nfrom sklearn.preprocessing import LabelEncoder # Helpful for encoding categorical labels (e.g., writer IDs) as numerical values.\nfrom sklearn.model_selection import train_test_split #  Used to split your data into training and testing sets.\nfrom PIL import Image # Part of the Python Imaging Library (PIL) and widely used for image processing and manipulation.\nfrom random import sample # Randomly selecting samples from your dataset.\nfrom tensorflow.keras.utils import to_categorical # Used for one-hot encoding categorical labels.\nfrom sklearn.utils import shuffle # Shuffling dataset\nfrom tensorflow.image import resize # Resizing the images for pre-processing and feeding to model.\nfrom tensorflow.keras.models import Sequential, load_model # Required for building Sequential ML model and load saved model\nfrom tensorflow.keras import layers # Contains various layers for building neural networks.\nfrom tensorflow.keras.callbacks import ModelCheckpoint # Used for saving the model after every epochs.\nimport pandas as pd # Useful for data manipulation and analysis, Saving the trained model history as excel file.","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:09.546664Z","iopub.execute_input":"2023-09-26T03:18:09.547714Z","iopub.status.idle":"2023-09-26T03:18:09.560924Z","shell.execute_reply.started":"2023-09-26T03:18:09.547671Z","shell.execute_reply":"2023-09-26T03:18:09.559979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 2: Uploading the Dataset on directory and acessing it.**\n\nAfter uploading the dataset accessing the content stored.\n\nThese are the forms in the dataset for quick access from manipulation of the file names on each column. Let's create a dictionary with form and writer mapping.","metadata":{}},{"cell_type":"code","source":"d = {}\n\n# \"C:\\\\TCS_internship\\\\OCR\\Dataset\\\\formss.txt\"\nwith open(\"../input/iam-handwriting-top50/forms_for_parsing.txt\") as forms:\n    for line in forms:\n        key = line.split(\" \")[0]\n        writer = line.split(\" \")[1]\n        d[key] = writer\n\nprint(len(d.keys()))","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:09.562742Z","iopub.execute_input":"2023-09-26T03:18:09.563358Z","iopub.status.idle":"2023-09-26T03:18:09.581586Z","shell.execute_reply.started":"2023-09-26T03:18:09.563324Z","shell.execute_reply":"2023-09-26T03:18:09.580475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 3: Preprocessing the Data.**\n\nAfter fetching the dataset we will preprocess the data.\n\nAll file-names list and target-writer names list are created.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T11:35:37.940222Z","iopub.execute_input":"2023-09-23T11:35:37.940667Z","iopub.status.idle":"2023-09-23T11:35:37.989546Z","shell.execute_reply.started":"2023-09-23T11:35:37.940632Z","shell.execute_reply":"2023-09-23T11:35:37.988129Z"}}},{"cell_type":"code","source":"temp = []\ntarget_list = []\n# Getting the relative path below\n\n# Dataset\\\\data_subset\npath_to_files = os.path.join(\"../input/iam-handwriting-top50/data_subset/data_subset\", \"*\")\n# ../input/iam-handwriting-top50/data_subset/data_subset/a01-000u-s00-00.png\nfor file_name in sorted(glob.glob(path_to_files)):\n    # print(file_name) # ../input/iam-handwriting-top50/data_subset/data_subset/a01-000u-s00-00.png\n    temp.append(file_name) # ['../input/iam-handwriting-top50/data_subset/data_subset/a01-000u-s00-00.png', '../input/iam-handwriting-top50/data_subset/data_subset/a01-000u-s00-01.png', ...\n    image_name = file_name.split(\"/\")[-1] # a01-000u-s00-00.png\n    # print(image_name) # # a01-000u-s00-00.png\n    file, ext = os.path.splitext(image_name) # Split the extension from a pathname.\n    # print(file, ext) # It gives filename, extention\n    parts = file.split(\"-\")\n    # print(parts) # ['a01', '000u', 's00', '00']\n    form = parts[0] + \"-\" + parts[1]\n    # print(form) # a01-000u\n\n    for key in d:\n        if key == form:\n            target_list.append(str(d[form]))\n\n# print(target_list) # ['000', '000', '000', '000', '000', '000', '000', ...\n\n# print(temp)\nimg_files = np.array(temp)\nimg_targets = np.array(target_list)\nprint(f\"Shape of the image files:- {img_files.shape}\")\nprint(f\"Shape of the target image files:- {img_targets.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:09.583000Z","iopub.execute_input":"2023-09-26T03:18:09.584093Z","iopub.status.idle":"2023-09-26T03:18:10.513300Z","shell.execute_reply.started":"2023-09-26T03:18:09.584022Z","shell.execute_reply":"2023-09-26T03:18:10.512304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualising of images.**\nLet's visualize the image data.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T11:41:55.584563Z","iopub.execute_input":"2023-09-23T11:41:55.584943Z","iopub.status.idle":"2023-09-23T11:41:55.590390Z","shell.execute_reply.started":"2023-09-23T11:41:55.584912Z","shell.execute_reply":"2023-09-23T11:41:55.589404Z"}}},{"cell_type":"code","source":"# Visualizing the image data\n\n\nfor file_name in img_files[:4]:\n    img = plt.imread(file_name)\n    plt.figure(figsize = (10, 10))\n    plt.axis(\"on\") # plt.axis(\"off\")\n    plt.imshow(img, cmap = \"gray\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:10.516084Z","iopub.execute_input":"2023-09-26T03:18:10.516702Z","iopub.status.idle":"2023-09-26T03:18:11.407401Z","shell.execute_reply.started":"2023-09-26T03:18:10.516665Z","shell.execute_reply":"2023-09-26T03:18:11.406429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good to observe that there are no categorical data. So, normalisation is done using label encoder.","metadata":{}},{"cell_type":"code","source":"# Label Encoding the writer names for one hot encoding later\n\nencoder = LabelEncoder()\nencoded_Y = encoder.fit_transform(img_targets)\n\nprint(img_files[:1], img_targets[:5], encoded_Y[:5])","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:11.409157Z","iopub.execute_input":"2023-09-26T03:18:11.409862Z","iopub.status.idle":"2023-09-26T03:18:11.418159Z","shell.execute_reply.started":"2023-09-26T03:18:11.409824Z","shell.execute_reply":"2023-09-26T03:18:11.417196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Setp 4: Splitting the datset into train, test and validation datsets.**\n\nSplitting of data into training and validation sets for cross validation with 4:1:1 ratio.\n\nAfter splitting we have 3233 train images, 833 test and validation images","metadata":{}},{"cell_type":"code","source":"train_files, rem_files, train_targets, rem_targets = train_test_split(img_files, encoded_Y, train_size = 0.66, random_state = 52, shuffle = True)\n\nvalidation_files, test_files, validation_targets, test_targets = train_test_split(rem_files, rem_targets, train_size = 0.5, random_state = 24, shuffle = True)\n\nprint(\"#\" * 20)\nprint(f\"Train files shape:- {train_files.shape}\")\nprint(f\"Validation files shape:- {validation_files.shape}\")\nprint(f\"Test files shape:- {test_files.shape}\")\n\nprint(\"#\" * 20)\nprint(f\"Train targets shape:- {train_targets.shape}\")\nprint(f\"Validation targets shape:- {validation_targets.shape}\")\nprint(f\"Test targets shape:- {test_targets.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:11.419732Z","iopub.execute_input":"2023-09-26T03:18:11.420075Z","iopub.status.idle":"2023-09-26T03:18:11.431553Z","shell.execute_reply.started":"2023-09-26T03:18:11.420042Z","shell.execute_reply":"2023-09-26T03:18:11.430281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Generator Helper Function as Input to Model**\n\nWe take patches of data, each of size 113 x 113. This generator function is implemented for that purpose.","metadata":{}},{"cell_type":"code","source":"batch_size = 16\nnum_classes = 50 # Number of writers are 50.\n\ndef generate_data(samples, target_files, batch_size = batch_size, factor = 0.1):\n    num_samples = len(samples)\n    while True: # Loop forever so the generator never terminates\n        for offset in range(0, num_samples, batch_size):\n            batch_samples = samples[offset : offset + batch_size]\n            batch_targets = target_files[offset : offset + batch_size]\n\n            images = []\n            targets = []\n\n            for i in range(len(batch_samples)):\n                batch_sample = batch_samples[i]\n                batch_target = batch_targets[i]\n                im = Image.open(batch_sample)\n                cur_width = im.size[0]\n                cur_height = im.size[1]\n\n                # print(cur_width, cur_height)\n                height_fac = 113 / cur_height\n\n                new_width = int(cur_width * height_fac)\n                size = new_width, 113\n\n                # # PIL.Image.Resampling.LANCZOS\n                imresize = im.resize((size), Image.LANCZOS) ## Resize so height = 113 while keeping aspect ratio \n                now_width = imresize.size[0]\n                now_height = imresize.size[1]\n                # Generate crops of size 113x113 from this resized image and keep random 10% of crops\n\n                # total x start points are from 0 to width -113\n                avail_x_points = list(range(0, now_width - 113))\n\n                # Pick random x%\n                pick_num = int(len(avail_x_points) * factor)\n\n                # Now pick\n                random_startx = sample(avail_x_points, pick_num)\n\n                for start in random_startx:\n                    imcrop = imresize.crop((start, 0, start + 113, 113))\n                    images.append(np.asarray(imcrop))\n                    targets.append(batch_target)\n\n            # trim image to only see section with road\n            X_train = np.array(images)\n            y_train = np.array(targets)\n\n            # reshaping X_train for feeding in later\n            X_train = X_train.reshape(X_train.shape[0], 113, 113, 1)\n            #convert to float and normalize\n            X_train = X_train.astype(\"float32\")\n            X_train = X_train / 255 # Normalizing\n\n            # One hot encode y\n            y_train = to_categorical(y_train, num_classes)\n\n            yield shuffle(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:11.433554Z","iopub.execute_input":"2023-09-26T03:18:11.433907Z","iopub.status.idle":"2023-09-26T03:18:11.447662Z","shell.execute_reply.started":"2023-09-26T03:18:11.433874Z","shell.execute_reply":"2023-09-26T03:18:11.446592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For training and testing,  generator helper function is called with the intent of making train and test generator data.","metadata":{}},{"cell_type":"code","source":"# Generate train, test and validation data\n\ntrain_generator = generate_data(train_files, train_targets, batch_size = batch_size, factor = 0.3)\nvalidation_generator = generate_data(validation_files, validation_targets, batch_size = batch_size, factor = 0.3)\ntest_generator = generate_data(test_files, test_targets, batch_size = batch_size, factor = 0.1)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:11.449158Z","iopub.execute_input":"2023-09-26T03:18:11.449534Z","iopub.status.idle":"2023-09-26T03:18:11.461855Z","shell.execute_reply.started":"2023-09-26T03:18:11.449497Z","shell.execute_reply":"2023-09-26T03:18:11.460837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 5:\tCreating the defining the model/network architecture.**\n\nAdding the Pooling Layers, CNN Layers, Activation Functions, etc.\n\nBuilding tensorflow.keras model and printing the model summary","metadata":{}},{"cell_type":"code","source":"def resize_image(image):\n    return resize(image, [56, 56])\n\nrow, col, ch = 113, 113, 1 # Rows, columns, channels\n\nmodel = Sequential(\n    [\n        layers.ZeroPadding2D(padding = (1, 1), input_shape = (row, col, ch)),\n\n        # Resizing the data within the neural network.\n        layers.Lambda(resize_image, name = \"Image_resize\"), # resizing of the images allows easy computation.\n        \n        # CNN Model\n        layers.Convolution2D(filters = 32, kernel_size = (5, 5), strides = (2, 2), padding = \"same\", name = \"conv1\"),\n        layers.Activation(activation = \"relu\"),\n        layers.MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\", name = \"pool1\"),\n\n        layers.Convolution2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = \"same\", name = \"conv2\"),\n        layers.Activation(activation = \"relu\"),\n        layers.MaxPooling2D(pool_size = (2, 2), strides = (2, 2), name = \"pool2\"),\n\n        layers.Convolution2D(filters = 128, kernel_size = (3, 3), strides = (1, 1), padding = \"same\", name = \"conv3\"),\n        layers.Activation(activation = \"relu\"),\n        layers.MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = \"valid\", name = \"pool3\"),\n\n        layers.Flatten(),\n        layers.Dropout(rate = 0.5),\n\n        layers.Dense(units = 512, name = \"dense1\"),      \n        layers.Activation(activation = \"relu\"),\n        layers.Dropout(rate = 0.5),\n\n        layers.Dense(units = 256, name = \"dense2\"),\n        layers.Activation(activation = \"relu\"),\n        layers.Dropout(rate = 0.5),\n\n        layers.Dense(units = num_classes, name = \"output\"),\n        layers.Activation(activation = \"softmax\"), # Using Softmax activation since output is within 50 classes.\n    ]\n)\n\nmodel.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:11.463416Z","iopub.execute_input":"2023-09-26T03:18:11.463986Z","iopub.status.idle":"2023-09-26T03:18:11.635377Z","shell.execute_reply.started":"2023-09-26T03:18:11.463954Z","shell.execute_reply":"2023-09-26T03:18:11.634628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Step 6: Training the model**\n\nUsing 8 epochs of 3000 train samples and 280 validation samples.","metadata":{}},{"cell_type":"code","source":"# Define the directory path within /kaggle/output/\ncheckpoint_directory = '/kaggle/working/checkpoint_1/' # /kaggle/output/checkpoint_1/\n\n# Create the directory if it doesn't exist\nif not os.path.exists(checkpoint_directory):\n    os.makedirs(checkpoint_directory)  # Use os.makedirs() to create parent directories if needed\n\n# Now you can save your data or model checkpoints to the checkpoint_directory\n# For example, if you want to save a model checkpoint:\n# model.save(os.path.join(checkpoint_directory, 'my_model.h5'))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:11.639183Z","iopub.execute_input":"2023-09-26T03:18:11.639545Z","iopub.status.idle":"2023-09-26T03:18:11.659858Z","shell.execute_reply.started":"2023-09-26T03:18:11.639512Z","shell.execute_reply":"2023-09-26T03:18:11.658614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\n\nepochs = 4 # 8 -> Accuracy :- 0.90\nsteps_per_epoch = 100 # 3000 # 3268\nvalidation_steps = 20 # 280 # 842\n\n# Saving every model using tensorflow.keras checkpoint\nfilepath = \"/kaggle/working/checkpoint_1/check-{epoch:02d}-{val_loss:.4f}.hdf5\" # Use .keras to remove warning \ncheckpoint = ModelCheckpoint(filepath = filepath, verbose = 1, save_best_only = False)\ncallbacks_list = [checkpoint]\n\n# fit_generator() is depricated therefore fit() will work\nhistory_object = model.fit(train_generator, steps_per_epoch = steps_per_epoch, \n                           validation_data = validation_generator,\n                           validation_steps = validation_steps, epochs = epochs,\n                           verbose = 1, callbacks = callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:18:11.660817Z","iopub.execute_input":"2023-09-26T03:18:11.661155Z","iopub.status.idle":"2023-09-26T03:27:35.549285Z","shell.execute_reply.started":"2023-09-26T03:18:11.661122Z","shell.execute_reply":"2023-09-26T03:27:35.548217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the history object in excel file using pandas. \n# pip install openpyxl \n\nhistory_dict = {\n    'epoch': list(range(1, len(history_object.history['loss']) + 1)),\n    'loss': history_object.history['loss'],\n    'accuracy': history_object.history['accuracy'],\n    'val_loss': history_object.history['val_loss'],\n    'val_accuracy': history_object.history['val_accuracy']\n}\nhistory_df = pd.DataFrame(history_dict)\nexcel_file_path = '/kaggle/working/training_history.xlsx'  # Replace with your desired file path\nhistory_df.to_excel(excel_file_path, index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:27:35.551108Z","iopub.execute_input":"2023-09-26T03:27:35.551497Z","iopub.status.idle":"2023-09-26T03:27:35.894479Z","shell.execute_reply.started":"2023-09-26T03:27:35.551445Z","shell.execute_reply":"2023-09-26T03:27:35.893509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 7: Saving the ML model**\nSaving the model using HDF5 file extension.","metadata":{}},{"cell_type":"code","source":"# Saving the whole model (Architecture, weights and configurations)\n\nmodel.save(\"/kaggle/working/HTR_model.hdf5\")\nprint(\"Whole model is saved successfully\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:27:35.895756Z","iopub.execute_input":"2023-09-26T03:27:35.896349Z","iopub.status.idle":"2023-09-26T03:27:35.957271Z","shell.execute_reply.started":"2023-09-26T03:27:35.896312Z","shell.execute_reply":"2023-09-26T03:27:35.956136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the whole model (Architecture, weights and configurations)\n\nloaded_model = load_model(\"/kaggle/working/HTR_model.hdf5\")\nprint(\"Whole model loaded successfully\")\nprint(loaded_model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:27:35.958929Z","iopub.execute_input":"2023-09-26T03:27:35.959307Z","iopub.status.idle":"2023-09-26T03:27:36.237497Z","shell.execute_reply.started":"2023-09-26T03:27:35.959271Z","shell.execute_reply":"2023-09-26T03:27:36.236721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 8: Testing the model**","metadata":{}},{"cell_type":"code","source":"# Testing the loaded model\n\n# evaluate_generator() is deprecated use evaluate() method instead.\nscores = loaded_model.evaluate_generator(test_generator , 280) # 842\nprint(scores)\nprint(f\"Model loss:- {scores[0]}\")\nprint(f\"Model Accuracy:- {scores[1]}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:27:36.238570Z","iopub.execute_input":"2023-09-26T03:27:36.238934Z","iopub.status.idle":"2023-09-26T03:29:58.833449Z","shell.execute_reply.started":"2023-09-26T03:27:36.238899Z","shell.execute_reply":"2023-09-26T03:29:58.832405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 9: Prediction**\nPrediction is done on test files","metadata":{}},{"cell_type":"code","source":"# Doing some preprocessing on test files\n\nimages = []\n\nfor filename in test_files[:50]:\n    im = Image.open(filename)\n    cur_width = im.size[0]\n    cur_height = im.size[1]\n\n    # print(cur_width, cur_height)\n\n    height_fac = 113 / cur_height\n\n    new_width = int(cur_width * height_fac)\n    size = new_width, 113\n    \n    imresize = im.resize((size), Image.LANCZOS) # Resizing so height = 113 while keeping aspect ratio\n    now_width = imresize.size[0]\n    now_height = imresize.size[1]\n    # Generating crops of size 113 x 113 from this resized image and keep random 10% of crops\n\n    # Total x start points are from 0 to width -113\n    avail_x_points = list(range(0, now_width - 113))\n\n    # Pick random x%\n    factor = 0.1\n    pick_num = int(len(avail_x_points) * factor)\n\n    random_startx = sample(avail_x_points, pick_num)\n\n    for start in random_startx:\n        imcrop = imresize.crop((start, 0, start + 113, 113))\n        images.append(np.asarray(imcrop))\n\n    X_test = np.array(images)\n\n    X_test = X_test.reshape(X_test.shape[0], 113, 113, 1)\n    # Convet to float and normalize\n    X_test = X_test.astype(\"float32\")\n    X_test /= 255\n    shuffle(X_test)\n\n\nprint(X_test.shape)    ","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:29:58.835273Z","iopub.execute_input":"2023-09-26T03:29:58.835654Z","iopub.status.idle":"2023-09-26T03:30:04.394202Z","shell.execute_reply.started":"2023-09-26T03:29:58.835618Z","shell.execute_reply":"2023-09-26T03:30:04.393071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictions","metadata":{}},{"cell_type":"code","source":"predictions = loaded_model.predict(X_test, verbose = 1)\n\nprint(predictions)\nprint(predictions.shape)\n\npredicted_writer = []\n\nfor pred in predictions:\n    predicted_writer.append(np.argmax(pred))\n\n# print(predicted_writer)\nprint(len(predicted_writer))","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:30:04.395824Z","iopub.execute_input":"2023-09-26T03:30:04.396181Z","iopub.status.idle":"2023-09-26T03:30:05.784586Z","shell.execute_reply.started":"2023-09-26T03:30:04.396147Z","shell.execute_reply":"2023-09-26T03:30:05.783507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 10: Plotting the loss and accuracy plots**\n\nLoss vs Epochs line Plot\nIt shows decrease in loss over epochs.\n\nAccuracy vs Epochs line plot\nIt shows increase in accuracy over epochs.","metadata":{}},{"cell_type":"code","source":"# Extract data from the history object\nacc = history_object.history[\"accuracy\"]\nval_acc = history_object.history[\"val_accuracy\"]\nloss = history_object.history[\"loss\"]\nval_loss = history_object.history[\"val_loss\"]\nepochs = range(1, len(loss) + 1)\n\n# Plot training and validation accuracy\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, acc, \"b\", label=\"Training Accuracy\")\nplt.plot(epochs, val_acc, \"r\", label=\"Validation Accuracy\")\nplt.title(\"Training and Validation Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\n# Plot training and validation loss\nplt.subplot(1, 2, 2)\nplt.plot(epochs, loss, \"b\", label=\"Training Loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"Validation Loss\")\nplt.title(\"Training and Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n# Save the plot in /kaggle/output/\nplot_filename = '/kaggle/working/Accuracy_Loss_Plot.png'  # Specify the desired file name and extension\nplt.savefig(plot_filename)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:30:05.786108Z","iopub.execute_input":"2023-09-26T03:30:05.786961Z","iopub.status.idle":"2023-09-26T03:30:06.687299Z","shell.execute_reply.started":"2023-09-26T03:30:05.786922Z","shell.execute_reply":"2023-09-26T03:30:06.686376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downloading all data of /kaggle/working/\n# Define the directory you want to zip and download\ndirectory_to_zip = '/kaggle/working/'\n\n# Specify the name for the zip file\nzip_file_name = 'working_directory'\n\n# Create a zip archive of the directory using the 'zip' shell command\n!zip -r {zip_file_name}.zip {directory_to_zip}\n\n# Display the download link for the zip file\nfrom IPython.display import FileLink\nFileLink(f'{zip_file_name}.zip')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T03:30:06.688362Z","iopub.execute_input":"2023-09-26T03:30:06.688715Z","iopub.status.idle":"2023-09-26T03:30:10.055539Z","shell.execute_reply.started":"2023-09-26T03:30:06.688682Z","shell.execute_reply":"2023-09-26T03:30:10.054370Z"},"trusted":true},"execution_count":null,"outputs":[]}]}